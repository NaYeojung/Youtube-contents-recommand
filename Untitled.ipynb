{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933c2001-11d0-45e4-a5c2-1598ed06792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import yt_dlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404ca82-0597-4b0a-9c55-e6f52fe20554",
   "metadata": {},
   "source": [
    "## ì±„ë„url ì…ë ¥ì‹œ ì±„ë„ì˜ìƒ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba119b2-f212-4d61-864d-8c320f6e3485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel Meta: {'channel_title': 'ìœ ë³‘ì¬', 'channel_id': 'UCHw9p667e9l0qoYfY8calaA', 'channel_url': 'https://www.youtube.com/@KoreanCryingGuy', 'subscriber_count': 1680000}\n",
      "Collected: 10\n",
      "                                    title     video_id  \\\n",
      "0         [ë¬´ê³µí•´] í—˜í•œ ê²ƒì´ ë‚˜ì™€ë„ ê¹€ê³ ì€ì´ë‘ ë¬´ì¡°ê±´ ê³µê°í•©ë‹ˆë‹¤  _ttuPeDExTo   \n",
      "1    [ë¬´ê³µí•´] í­ì£¼í•˜ëŠ” ê³µê° ìš”ì •.. ì•„ì´ë¸Œ ë ˆì´ë‘ ë¬´ì¡°ê±´ ê³µê°í•©ë‹ˆë‹¤  N5Zk-xH1e0k   \n",
      "2  [ENG SUB] ì§ì—­ëœ ê°€ì‚¬ë³´ê³  ë…¸ë˜ ë§íˆê¸° (w.íƒœë˜ ê·œë¹ˆ ë¦¬í‚¤)  -6vOqZs6CFA   \n",
      "3                             ê·¼ë° ìš´ì „ì€ ë¡œì´í‚´ì´  QxSIqNOMCm4   \n",
      "4         [ë¬´ê³µí•´] ìƒ¤ì´ë‹ˆ Të‘, ì•„ë‹ˆ í‚¤ë‘.. ë¬´ì¡°ê±´ ê³µê°í•©ë‹ˆë‹¤  cqIZU2iQym4   \n",
      "\n",
      "                                     video_url published_date  \\\n",
      "0  https://www.youtube.com/watch?v=_ttuPeDExTo     2025-09-12   \n",
      "1  https://www.youtube.com/watch?v=N5Zk-xH1e0k     2025-09-05   \n",
      "2  https://www.youtube.com/watch?v=-6vOqZs6CFA     2025-09-02   \n",
      "3  https://www.youtube.com/watch?v=QxSIqNOMCm4     2025-08-29   \n",
      "4  https://www.youtube.com/watch?v=cqIZU2iQym4     2025-08-20   \n",
      "\n",
      "                                       thumbnail_url  view_count  like_count  \\\n",
      "0  https://i.ytimg.com/vi/_ttuPeDExTo/maxresdefau...      483629        7348   \n",
      "1  https://i.ytimg.com/vi/N5Zk-xH1e0k/maxresdefau...      626912        8775   \n",
      "2  https://i.ytimg.com/vi/-6vOqZs6CFA/maxresdefau...      184211        8048   \n",
      "3  https://i.ytimg.com/vi_webp/QxSIqNOMCm4/maxres...      164028        1923   \n",
      "4  https://i.ytimg.com/vi/cqIZU2iQym4/maxresdefau...     1492312       21396   \n",
      "\n",
      "   comment_count duration_hms  duration_sec channel_name  \\\n",
      "0            422        36:18          2178          ìœ ë³‘ì¬   \n",
      "1            546        35:35          2135          ìœ ë³‘ì¬   \n",
      "2            511        21:34          1294          ìœ ë³‘ì¬   \n",
      "3            198        24:17          1457          ìœ ë³‘ì¬   \n",
      "4            880        33:40          2020          ìœ ë³‘ì¬   \n",
      "\n",
      "                 channel_id  subscriber_count categories  \\\n",
      "0  UCHw9p667e9l0qoYfY8calaA           1680000   [Comedy]   \n",
      "1  UCHw9p667e9l0qoYfY8calaA           1680000   [Comedy]   \n",
      "2  UCHw9p667e9l0qoYfY8calaA           1680000   [Comedy]   \n",
      "3  UCHw9p667e9l0qoYfY8calaA           1680000   [Comedy]   \n",
      "4  UCHw9p667e9l0qoYfY8calaA           1680000   [Comedy]   \n",
      "\n",
      "                                                tags  \\\n",
      "0  [ì½”ë¯¸ë””, ê°œê·¸, ë¼ì´ë¸Œì½”ë¯¸ë””, comedy, korea comedy, ìœ ë³‘ì¬, ë¬¸...   \n",
      "1  [ì½”ë¯¸ë””, ê°œê·¸, ë¼ì´ë¸Œì½”ë¯¸ë””, comedy, korea comedy, ìœ ë³‘ì¬, ë¬¸...   \n",
      "2  [ì½”ë¯¸ë””, ê°œê·¸, ë¼ì´ë¸Œì½”ë¯¸ë””, comedy, korea comedy, ìœ ë³‘ì¬, ë¬¸...   \n",
      "3  [ì½”ë¯¸ë””, ê°œê·¸, ë¼ì´ë¸Œì½”ë¯¸ë””, comedy, korea comedy, ìœ ë³‘ì¬, ë¬¸...   \n",
      "4  [ì½”ë¯¸ë””, ê°œê·¸, ë¼ì´ë¸Œì½”ë¯¸ë””, comedy, korea comedy, ìœ ë³‘ì¬, ë¬¸...   \n",
      "\n",
      "                               uploader_url  age_limit availability  \\\n",
      "0  https://www.youtube.com/@KoreanCryingGuy          0       public   \n",
      "1  https://www.youtube.com/@KoreanCryingGuy          0       public   \n",
      "2  https://www.youtube.com/@KoreanCryingGuy          0       public   \n",
      "3  https://www.youtube.com/@KoreanCryingGuy          0       public   \n",
      "4  https://www.youtube.com/@KoreanCryingGuy          0       public   \n",
      "\n",
      "  live_status                                        description  \n",
      "0    not_live  ë„·í”Œë¦­ìŠ¤ ì‹œë¦¬ì¦ˆ [ì€ì¤‘ê³¼ ìƒì—°] ë“œë””ì–´ ğŸ‘‰ì˜¤ëŠ˜ ê³µê°œ ğŸ‘ˆ\\n\\nì¹œêµ¬ë€ ê²Œ... ì°¸,\\...  \n",
      "1    not_live  *ì‚¬ì—°ìë“¤ì€ ëª¨ë‘ ì¼ë°˜ì¸ì…ë‹ˆë‹¤. ë¬´ë¶„ë³„í•œ ë¹„ë‚œì€ ì‚­ì œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤*\\n\\nê³µê°ì´ ...  \n",
      "2    not_live  ğŸ“Œhttps://bit.ly/3ULB2X7\\nğŸ‘‰ ìŠ¤í”½XT1 ì½”ìŠ¤ë¥¼ ë¬´ë£Œ ì²´í—˜ìœ¼ë¡œ ë”±...  \n",
      "3    not_live  ì˜ì¹´ëŠ” ë³´ì´ëŠ” ê°€ê²© ê·¸ëŒ€ë¡œğŸ˜¶\\nì´ì œ [ì´ ê²°ì œ ìš”ê¸ˆ] í•œ ë²ˆë§Œ ë‚´ê³  íƒ€ì„¸ìš”\\n\\n5...  \n",
      "4    not_live  [8/6(ìˆ˜)ì— ì§„í–‰í•œ ë¼ì´ë¸Œ ë°©ì†¡ì…ë‹ˆë‹¤]\\n*ì‚¬ì—°ìë“¤ì€ ëª¨ë‘ ì¼ë°˜ì¸ì…ë‹ˆë‹¤. ë¬´ë¶„ë³„...  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# ê³µí†µ ìœ í‹¸\n",
    "# ------------------------------\n",
    "def _sec_to_hms(sec: Optional[int]) -> Optional[str]:\n",
    "    if sec is None:\n",
    "        return None\n",
    "    m, s = divmod(int(sec), 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1) ì±„ë„ ì—…ë¡œë“œ ëª©ë¡ì—ì„œ ì˜ìƒ URL ìˆ˜ì§‘\n",
    "# ------------------------------\n",
    "def list_channel_video_urls(\n",
    "    channel_url: str,\n",
    "    max_videos: Optional[int] = None,\n",
    ") -> Tuple[List[str], Dict]:\n",
    "    \"\"\"\n",
    "    yt_dlpë¡œ ì±„ë„ ì—…ë¡œë“œ ëª©ë¡(playlistì²˜ëŸ¼ ë™ì‘)ì„ 'í‰ë©´ ì¶”ì¶œ'í•´ì„œ ì˜ìƒ URLë§Œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘.\n",
    "    í•„ìš”ì‹œ '/videos' íƒ­ìœ¼ë¡œ ì¬ì‹œë„.\n",
    "    ë°˜í™˜: (video_urls, channel_meta)\n",
    "    \"\"\"\n",
    "    def _extract_entries(url: str):\n",
    "        ydl_opts = {\n",
    "            \"quiet\": True,\n",
    "            \"no_warnings\": True,\n",
    "            \"skip_download\": True,\n",
    "            \"noplaylist\": False,          # ì±„ë„ì€ ë‚´ë¶€ì ìœ¼ë¡œ playlistì²˜ëŸ¼ ì²˜ë¦¬\n",
    "            \"extract_flat\": True,         # ëª©ë¡ë§Œ ë¹ ë¥´ê²Œ\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            return ydl.extract_info(url, download=False)\n",
    "\n",
    "    info = None\n",
    "    try:\n",
    "        info = _extract_entries(channel_url)\n",
    "    except Exception:\n",
    "        # /videos íƒ­ìœ¼ë¡œ 1ì°¨ ë³´ì •\n",
    "        if \"/videos\" not in channel_url:\n",
    "            fixed = channel_url.rstrip(\"/\") + \"/videos\"\n",
    "            info = _extract_entries(fixed)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # ì—”íŠ¸ë¦¬ ì—†ìœ¼ë©´ /videosë¡œ ìµœì¢… ì¬ì‹œë„\n",
    "    entries = (info or {}).get(\"entries\") or []\n",
    "    if not entries and \"/videos\" not in channel_url:\n",
    "        fixed = channel_url.rstrip(\"/\") + \"/videos\"\n",
    "        info = _extract_entries(fixed)\n",
    "        entries = (info or {}).get(\"entries\") or []\n",
    "\n",
    "    if not entries:\n",
    "        raise RuntimeError(\"ì±„ë„ ì—…ë¡œë“œ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì±„ë„ URLì´ ë§ëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜ '/videos' íƒ­ URLì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
    "\n",
    "    # ì±„ë„ ë©”íƒ€\n",
    "    channel_meta = {\n",
    "        \"channel_title\": info.get(\"channel\") or info.get(\"uploader\"),\n",
    "        \"channel_id\": info.get(\"channel_id\") or info.get(\"uploader_id\"),\n",
    "        \"channel_url\": info.get(\"uploader_url\") or channel_url,\n",
    "        \"subscriber_count\": info.get(\"channel_follower_count\"),\n",
    "    }\n",
    "\n",
    "    # í‰ë©´ ì—”íŠ¸ë¦¬ â†’ ë™ì˜ìƒ URL ìƒì„±\n",
    "    urls: List[str] = []\n",
    "    for e in entries:\n",
    "        if isinstance(e, dict):\n",
    "            # e[\"url\"]ì´ idë§Œ ë“¤ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ë³´ì •\n",
    "            raw = e.get(\"url\") or e.get(\"id\")\n",
    "            if not raw:\n",
    "                continue\n",
    "            full = raw if str(raw).startswith(\"http\") else f\"https://www.youtube.com/watch?v={raw}\"\n",
    "            # Shorts ë“±ë„ ì—…ë¡œë“œ ëª©ë¡ì— í¬í•¨ë  ìˆ˜ ìˆìŒ(í•„í„°ëŠ” ìƒì„¸ ë‹¨ê³„ì—ì„œ ê°€ëŠ¥)\n",
    "            urls.append(full)\n",
    "        elif isinstance(e, str):\n",
    "            urls.append(f\"https://www.youtube.com/watch?v={e}\")\n",
    "\n",
    "        if max_videos and len(urls) >= max_videos:\n",
    "            break\n",
    "\n",
    "    # ì¤‘ë³µ ì œê±°(ê°„í˜¹ ì¤‘ë³µì´ ì„ì´ëŠ” ê²½ìš° ë°©ì§€)\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    return urls, channel_meta\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2) ë‹¨ì¼ ì˜ìƒ ìƒì„¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
    "# ------------------------------\n",
    "def get_youtube_video_info(video_url: str) -> Dict:\n",
    "    ydl_opts = {\n",
    "        \"noplaylist\": True,\n",
    "        \"quiet\": True,\n",
    "        \"no_warnings\": True,\n",
    "        \"skip_download\": True,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(video_url, download=False)\n",
    "\n",
    "    # ë‚ ì§œ ISO ë³€í™˜\n",
    "    upload_date = info.get(\"upload_date\")\n",
    "    published_date = None\n",
    "    if upload_date:\n",
    "        try:\n",
    "            published_date = datetime.strptime(upload_date, \"%Y%m%d\").date().isoformat()\n",
    "        except Exception:\n",
    "            published_date = upload_date  # ì›ë³¸ ìœ ì§€\n",
    "\n",
    "    # ê°€ì¥ í° ì¸ë„¤ì¼\n",
    "    thumbnail_url = None\n",
    "    thumbs = info.get(\"thumbnails\")\n",
    "    if isinstance(thumbs, list) and thumbs:\n",
    "        thumbnail_url = sorted(thumbs, key=lambda t: t.get(\"height\", 0))[-1].get(\"url\")\n",
    "\n",
    "    duration_str = info.get(\"duration_string\") or _sec_to_hms(info.get(\"duration\"))\n",
    "    channel_name = info.get(\"channel\") or info.get(\"uploader\")\n",
    "\n",
    "    return {\n",
    "        \"title\": info.get(\"title\"),\n",
    "        \"video_id\": info.get(\"id\"),\n",
    "        \"video_url\": f\"https://www.youtube.com/watch?v={info.get('id')}\" if info.get(\"id\") else video_url,\n",
    "        \"published_date\": published_date,\n",
    "        \"thumbnail_url\": thumbnail_url,\n",
    "        \"view_count\": info.get(\"view_count\"),\n",
    "        \"like_count\": info.get(\"like_count\"),\n",
    "        \"comment_count\": info.get(\"comment_count\"),\n",
    "        \"duration_hms\": duration_str,\n",
    "        \"duration_sec\": info.get(\"duration\"),\n",
    "        \"channel_name\": channel_name,\n",
    "        \"channel_id\": info.get(\"channel_id\") or info.get(\"uploader_id\"),\n",
    "        \"subscriber_count\": info.get(\"channel_follower_count\"),\n",
    "\n",
    "        # ì„ íƒ í•„ë“œ\n",
    "        \"categories\": info.get(\"categories\"),\n",
    "        \"tags\": info.get(\"tags\"),\n",
    "        \"uploader_url\": info.get(\"uploader_url\"),\n",
    "        \"age_limit\": info.get(\"age_limit\"),\n",
    "        \"availability\": info.get(\"availability\"),\n",
    "        \"live_status\": info.get(\"live_status\"),\n",
    "        \"description\": info.get(\"description\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3) ì±„ë„ â†’ (URL ìˆ˜ì§‘) â†’ (ê° ì˜ìƒ ìƒì„¸ ìˆ˜ì§‘)\n",
    "# ------------------------------\n",
    "def collect_channel_videos(\n",
    "    channel_url: str,\n",
    "    max_videos: Optional[int] = None,\n",
    "    include_shorts: bool = True,\n",
    "    sleep_range: Tuple[float, float] = (0.05, 0.2),\n",
    "    retry: int = 2,\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    channel_url: ì±„ë„ í™ˆ/í•¸ë“¤/UCID URL ëª¨ë‘ ê°€ëŠ¥. í•„ìš”ì‹œ '/videos'ë¡œ ìë™ ë³´ì •\n",
    "    max_videos: ìˆ˜ì§‘ ê°œìˆ˜ ì œí•œ(Noneì´ë©´ ì „ì²´)\n",
    "    include_shorts: Falseë©´ duration<60ì¸ ì˜ìƒ(ìˆì¸ ) ì œì™¸\n",
    "    sleep_range: ìš”ì²­ ê°„ ëœë¤ ì§€ì—°(ì°¨ë‹¨ ë°©ì§€)\n",
    "    retry: ì˜ìƒë³„ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜\n",
    "    \"\"\"\n",
    "    video_urls, channel_meta = list_channel_video_urls(channel_url, max_videos=max_videos)\n",
    "\n",
    "    rows = []\n",
    "    for url in video_urls:\n",
    "        last_err = None\n",
    "        for attempt in range(retry + 1):\n",
    "            try:\n",
    "                row = get_youtube_video_info(url)\n",
    "                # ìˆì¸  ì œì™¸ ì˜µì…˜\n",
    "                if not include_shorts:\n",
    "                    dur = row.get(\"duration_sec\")\n",
    "                    if dur is not None and int(dur) < 60:\n",
    "                        # skip\n",
    "                        pass\n",
    "                    else:\n",
    "                        rows.append(row)\n",
    "                else:\n",
    "                    rows.append(row)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                if attempt < retry:\n",
    "                    time.sleep(0.5 + attempt * 0.5)\n",
    "                else:\n",
    "                    # ì‹¤íŒ¨í•´ë„ ìµœì†Œ ì •ë³´ ê¸°ë¡\n",
    "                    rows.append({\n",
    "                        \"title\": None,\n",
    "                        \"video_id\": None,\n",
    "                        \"video_url\": url,\n",
    "                        \"published_date\": None,\n",
    "                        \"thumbnail_url\": None,\n",
    "                        \"view_count\": None,\n",
    "                        \"like_count\": None,\n",
    "                        \"comment_count\": None,\n",
    "                        \"duration_hms\": None,\n",
    "                        \"duration_sec\": None,\n",
    "                        \"channel_name\": channel_meta.get(\"channel_title\"),\n",
    "                        \"channel_id\": channel_meta.get(\"channel_id\"),\n",
    "                        \"subscriber_count\": channel_meta.get(\"subscriber_count\"),\n",
    "                        \"_error\": str(last_err),\n",
    "                    })\n",
    "        time.sleep(random.uniform(*sleep_range))\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ì •ë ¬(ê°€ëŠ¥í•˜ë©´)\n",
    "    if \"published_date\" in df.columns:\n",
    "        try:\n",
    "            df[\"published_date\"] = pd.to_datetime(df[\"published_date\"], errors=\"coerce\")\n",
    "            df = df.sort_values(\"published_date\", ascending=False).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return df, channel_meta\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    channel_url = \"https://www.youtube.com/@KoreanCryingGuy/videos\"\n",
    "\n",
    "    # ì˜µì…˜\n",
    "    MAX_VIDEOS = 10\n",
    "    INCLUDE_SHORTS = False\n",
    "    SAVE_CSV = False\n",
    "    CSV_PATH = \"channel_videos_metadata.csv\"\n",
    "\n",
    "    df, meta = collect_channel_videos(\n",
    "        channel_url=channel_url,\n",
    "        max_videos=MAX_VIDEOS,\n",
    "        include_shorts=INCLUDE_SHORTS,\n",
    "        sleep_range=(0.08, 0.25),\n",
    "        retry=2\n",
    "    )\n",
    "\n",
    "    print(\"Channel Meta:\", meta)\n",
    "    print(\"Collected:\", len(df))\n",
    "    print(df.head(5))\n",
    "\n",
    "    if SAVE_CSV:\n",
    "        df.to_csv(CSV_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Saved -> {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "af42a8ae-f26f-40d8-824c-38d6339466a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import yt_dlp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ---------- ê°„ë‹¨ í† í¬ë‚˜ì´ì € & TF-IDF ----------\n",
    "def tokenize_ko(text: str):\n",
    "    text = re.sub(r\"[^0-9ê°€-í£A-Za-z]+\", \" \", str(text))\n",
    "    toks = text.lower().split()\n",
    "    return [t for t in toks if len(t) > 1]\n",
    "\n",
    "STOPWORDS = set([\"ì˜ìƒ\",\"ì±„ë„\",\"ìœ íŠœë¸Œ\"])\n",
    "\n",
    "def analyzer(doc: str):\n",
    "    return [t for t in tokenize_ko(doc) if t not in STOPWORDS]\n",
    "\n",
    "def tfidf_top_keywords(titles, top_k=20, min_df=2):\n",
    "    vec = TfidfVectorizer(analyzer=analyzer, ngram_range=(1,2), min_df=min_df)\n",
    "    X = vec.fit_transform(pd.Series(titles).astype(str))\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    scores = np.asarray(X.sum(axis=0)).ravel()\n",
    "    order = scores.argsort()[::-1][:top_k]\n",
    "    return list(zip(vocab[order], scores[order]))\n",
    "\n",
    "# ---------- yt-dlp ê³µí†µ í´ë¼ì´ì–¸íŠ¸ ----------\n",
    "def ydl_client(cookies_path=None, region=\"KR\"):\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"extract_flat\": True,          # ëª©ë¡ì„ ë‚¼ ë•Œ í•„ìˆ˜\n",
    "        \"noplaylist\": False,\n",
    "        \"ignoreerrors\": True,\n",
    "        \"geo_bypass\": True,\n",
    "        # Accept-Language ê°•ì œ\n",
    "        \"http_headers\": {\"Accept-Language\": \"ko-KR,ko;q=0.9\"},\n",
    "    }\n",
    "    if cookies_path:\n",
    "        ydl_opts[\"cookiefile\"] = cookies_path\n",
    "    return yt_dlp.YoutubeDL(ydl_opts)\n",
    "\n",
    "def get_full_info(url_or_id: str, cookies_path=None):\n",
    "    # ê°œë³„ ì˜ìƒì˜ ìƒì„¸(ì¹´í…Œê³ ë¦¬ ë“±)ë¥¼ ë‹¤ì‹œ ê¸ì„ ë•ŒëŠ” extract_flat=Falseë¡œ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    with yt_dlp.YoutubeDL({\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"extract_flat\": False,\n",
    "        \"ignoreerrors\": True,\n",
    "        \"http_headers\": {\"Accept-Language\": \"ko-KR,ko;q=0.9\"},\n",
    "        **({\"cookiefile\": cookies_path} if cookies_path else {})\n",
    "    }) as ydl:\n",
    "        return ydl.extract_info(url_or_id, download=False)\n",
    "\n",
    "def extract_category(info: dict):\n",
    "    if not isinstance(info, dict):\n",
    "        return None\n",
    "    if info.get(\"categories\"):\n",
    "        cats = info[\"categories\"]\n",
    "        return cats[0] if isinstance(cats, list) and cats else None\n",
    "    if info.get(\"category\"):\n",
    "        return info[\"category\"]\n",
    "    return None\n",
    "\n",
    "# ---------- ì±„ë„ì˜ ì£¼ ì¹´í…Œê³ ë¦¬ ì¶”ì •(ì„ íƒ) ----------\n",
    "def infer_channel_main_category(df_channel: pd.DataFrame, url_col=\"url\", cookies_path=None):\n",
    "    if url_col not in df_channel.columns:\n",
    "        return None\n",
    "    cats = []\n",
    "    for u in tqdm(df_channel[url_col].astype(str).tolist(), desc=\"Channel video categories\"):\n",
    "        try:\n",
    "            info = get_full_info(u, cookies_path=cookies_path)\n",
    "            cat = extract_category(info)\n",
    "            if cat:\n",
    "                cats.append(cat)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cats:\n",
    "        return None\n",
    "    return collections.Counter(cats).most_common(1)[0][0]\n",
    "\n",
    "# ---------- íŠ¸ë Œë”© ìˆ˜ì§‘ (ì¹´í…Œê³ ë¦¬ í¬í•¨) ----------\n",
    "def get_trending_with_categories(region=\"KR\", limit=30, cookies_path=None):\n",
    "    # ì§€ì—­/ì–¸ì–´ ê³ ì • íŒŒë¼ë¯¸í„°ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë°©ì§€\n",
    "    trending_url = f\"https://www.youtube.com/feed/trending?gl={region}&persist_gl=1&hl=ko\"\n",
    "    with ydl_client(cookies_path=cookies_path, region=region) as ydl:\n",
    "        listing = ydl.extract_info(trending_url, download=False)\n",
    "    entries = (listing or {}).get(\"entries\", [])[:limit]\n",
    "\n",
    "    rows = []\n",
    "    for e in tqdm(entries, desc=\"Trending details\"):\n",
    "        vid = e.get(\"id\") or e.get(\"url\")\n",
    "        if not vid:\n",
    "            continue\n",
    "        video_url = f\"https://www.youtube.com/watch?v={vid}\"\n",
    "        try:\n",
    "            info = get_full_info(video_url, cookies_path=cookies_path)\n",
    "            title = info.get(\"title\") or e.get(\"title\")\n",
    "            cat = extract_category(info)\n",
    "            rows.append({\n",
    "                \"video_id\": info.get(\"id\") or vid,\n",
    "                \"title\": title,\n",
    "                \"category\": cat,\n",
    "                \"url\": video_url\n",
    "            })\n",
    "            print(pd.DataFrame(rows))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- íŒŒì´í”„ë¼ì¸: ê°™ì€ ì¹´í…Œê³ ë¦¬ë§Œ ëª¨ì•„ TF-IDF ----------\n",
    "def tfidf_with_same_category(df_channel: pd.DataFrame,\n",
    "                             channel_category: str = None,\n",
    "                             region=\"KR\",\n",
    "                             trending_limit=40,\n",
    "                             url_col=\"url\",\n",
    "                             top_k=20,\n",
    "                             cookies_path=None):\n",
    "    if channel_category is None:\n",
    "        channel_category = infer_channel_main_category(df_channel, url_col=url_col, cookies_path=cookies_path)\n",
    "    if not channel_category:\n",
    "        raise ValueError(\"ì±„ë„ ì¹´í…Œê³ ë¦¬ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. channel_categoryë¥¼ ì§ì ‘ ì§€ì •í•˜ê±°ë‚˜ df_channelì— urlì„ ì œê³µí•˜ì„¸ìš”.\")\n",
    "\n",
    "    df_trend = get_trending_with_categories(region=region, limit=trending_limit, cookies_path=cookies_path)\n",
    "    df_trend_same = df_trend[df_trend[\"category\"] == channel_category].copy()\n",
    "\n",
    "    titles_all = pd.concat([\n",
    "        df_channel[\"title\"].astype(str),\n",
    "        df_trend_same[\"title\"].astype(str)\n",
    "    ]).tolist()\n",
    "\n",
    "    keywords = tfidf_top_keywords(titles_all, top_k=top_k, min_df=2)\n",
    "    return {\n",
    "        \"channel_category\": channel_category,\n",
    "        \"n_trending_all\": len(df_trend),\n",
    "        \"n_trending_same_cat\": len(df_trend_same),\n",
    "        \"top_keywords\": keywords,\n",
    "        \"df_trending_same\": df_trend_same\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f126efbb-97bf-4b5a-ab61-d23b33996339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: [youtube:tab] trending: The channel/playlist does not exist and the URL redirected to youtube.com home page\n",
      "Trending details: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'category'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m df_channel \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_url\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1) ì±„ë„ ì¹´í…Œê³ ë¦¬ ìë™ ì¶”ì • + ë™ì¼ ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë”© ê²°í•© TF-IDF\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m tfidf_with_same_category(\n\u001b[0;32m      7\u001b[0m     df_channel,\n\u001b[0;32m      8\u001b[0m     channel_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComedy\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# Noneì´ë©´ df_channel urlë¡œ ì¶”ì • ì‹œë„\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     region\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     trending_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m     11\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mì±„ë„ ì£¼ ì¹´í…Œê³ ë¦¬:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannel_category\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124míŠ¸ë Œë”© ì „ì²´ ê°œìˆ˜:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_trending_all\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[126], line 126\u001b[0m, in \u001b[0;36mtfidf_with_same_category\u001b[1;34m(df_channel, channel_category, region, trending_limit, url_col, top_k, cookies_path)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mì±„ë„ ì¹´í…Œê³ ë¦¬ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. channel_categoryë¥¼ ì§ì ‘ ì§€ì •í•˜ê±°ë‚˜ df_channelì— urlì„ ì œê³µí•˜ì„¸ìš”.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m df_trend \u001b[38;5;241m=\u001b[39m get_trending_with_categories(region\u001b[38;5;241m=\u001b[39mregion, limit\u001b[38;5;241m=\u001b[39mtrending_limit, cookies_path\u001b[38;5;241m=\u001b[39mcookies_path)\n\u001b[1;32m--> 126\u001b[0m df_trend_same \u001b[38;5;241m=\u001b[39m df_trend[df_trend[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m channel_category]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    128\u001b[0m titles_all \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[0;32m    129\u001b[0m     df_channel[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m),\n\u001b[0;32m    130\u001b[0m     df_trend_same[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    131\u001b[0m ])\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    133\u001b[0m keywords \u001b[38;5;241m=\u001b[39m tfidf_top_keywords(titles_all, top_k\u001b[38;5;241m=\u001b[39mtop_k, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'category'"
     ]
    }
   ],
   "source": [
    "# ================== ì‚¬ìš© ì˜ˆì‹œ ==================\n",
    "if __name__ == \"__main__\":\n",
    "    df_channel = df[['title','video_url']]\n",
    "\n",
    "    # 1) ì±„ë„ ì¹´í…Œê³ ë¦¬ ìë™ ì¶”ì • + ë™ì¼ ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë”© ê²°í•© TF-IDF\n",
    "    result = tfidf_with_same_category(\n",
    "        df_channel,\n",
    "        channel_category='Comedy',   # Noneì´ë©´ df_channel urlë¡œ ì¶”ì • ì‹œë„\n",
    "        region=\"KR\",\n",
    "        trending_limit=40,\n",
    "        top_k=20\n",
    "    )\n",
    "\n",
    "    print(\"\\nì±„ë„ ì£¼ ì¹´í…Œê³ ë¦¬:\", result[\"channel_category\"])\n",
    "    print(\"íŠ¸ë Œë”© ì „ì²´ ê°œìˆ˜:\", result[\"n_trending_all\"])\n",
    "    print(\"ë™ì¼ ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë”© ê°œìˆ˜:\", result[\"n_trending_same_cat\"])\n",
    "    print(\"\\n[Top TF-IDF í‚¤ì›Œë“œ]\")\n",
    "    for k, s in result[\"top_keywords\"]:\n",
    "        print(f\"{k}\\t{s:.3f}\")\n",
    "\n",
    "    # í•„ìš”í•˜ë©´ ë™ì¼ ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë”© ëª©ë¡ í™•ì¸\n",
    "    # print(result[\"df_trending_same\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae7134-9361-4867-87eb-de6dbcce8861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
